{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning with scikit-learn\n",
    "### Rachel Rakov\n",
    "Welcome!  In this workshop, we are going to learn how to go through the process of doing *machine learning* on a set of data.   To do so, we will download a *corpus* of text data to work with, extract *features* from this data, and do *supervised* machine learning to our data, using a mathmatical algorithm to train a *classifier* which will then classify previously unseen data into a set of predefined categories.\n",
    "\n",
    "\n",
    "\"Machine learning is a research field that sits at the intersections of statistics, artificial intelligence, and computer science.  It is also known as *predictive analystics* or *statistical learning*.\"\n",
    "\n",
    "-- Andreas Mueller, \"Introduction to Machine Learning with Python\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key terms\n",
    "- *machine learning*: An application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed\n",
    "\n",
    "\n",
    "- *corpus*: A large collection of data.  In our case, this will be text data (although a corpus can contain any type of data)\n",
    "\n",
    "\n",
    "- *features*: Properties that describe data entities for machine learning\n",
    "\n",
    "\n",
    "- *feature representation, feature vector*: A set of features\n",
    "\n",
    "\n",
    "- *supervised machine learning*:  A machine learning task of learning a function that maps an input to an output based on example input-output pairs\n",
    "\n",
    "\n",
    "- *unsupervised machine learning*: A machine learning task used to draw inferences from datasets consisting of input data without labeled responses (lacks input-output pairs; only has input data)\n",
    "\n",
    "\n",
    "- *algorithm*: A process or set of rules to be followed in calculations (or other problem-solving operations), particularly by a computer\n",
    "\n",
    "\n",
    "- *classification*: An machine learning task used to predict a class label, which is a choice from a predefined list of possibilities\n",
    "\n",
    "Sources: Wikipedia, Andreas Mueller's \"Introduction to Machine Learning with Python\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals of this workshop\n",
    "In this workshop, you will learn the following skills:\n",
    "- How to use skills from the NLTK workshop to build features for a classification task\n",
    "- How to build a text classification system that can predict whether sentences belong to one category (\"news\") or another (\"romance\")\n",
    "- How to prepare data for machine learning using *pandas*, a package for Python that helps to organize your data\n",
    "    - Looks similar to an Excel spreadsheet\n",
    "- How to use the scikit-learn package for Python to perform machine learning on the data\n",
    "- How to evaluate the results of the classifier, helping to decide whether the classifier is effective\n",
    "- How to adjust paramaters of a classifier to improve accuracy\n",
    "\n",
    "### What do you need for this workshop?\n",
    "- Python 3\n",
    "    - You can also download the Jupyter Notebook for this lesson to follow along\n",
    "- The Natural Lanugage Toolkit\n",
    "    - We will be using both corpora and tools from this package\n",
    "- pandas \n",
    "    - We will use this for data processing\n",
    "    - Comes with Anaconda\n",
    "- matplotlib\n",
    "    - We will use this for visualizing our data\n",
    "    - Comes with Anaconda\n",
    "- sckiit-learn\n",
    "    - We will use this for machine learning\n",
    "    - Comes with Anaconda \n",
    "\n",
    "### Let's get started by importing some packages we will need for this workshop!\n",
    "- The Brown Corpus: A text corpus of American English, split into fifteen different categories\n",
    "- Part of speech taggers (POS): prebuilt functions that are designed to determine the part of speech of every word in the sentence you give them\n",
    "- Pandas as pd: importing the Pandas toolkit and renaming it pd, so we don't have to type too much\n",
    "- matplotlib.pyplot as plt: importing plotting tools from matplotlib and renaming them plt\n",
    "    - ~~~\n",
    "    %matplotlib inline\n",
    "    ~~~\n",
    "    We use the above code to ensure our images display clearly in the Jupyter notebook.\n",
    "\n",
    "- sklearn: the scikit-learn machine learning toolkit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk import pos_tag_sents\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is classification?  Let's show an example using fruit!\n",
    "\n",
    "### How would you describe apples to a computer?  How would they differ from oranges?\n",
    "Remember, computers can only really understand numbers, true false values, and strings within a predefined set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fruit3](images/fruit3.png)\n",
    "Source: Andrew Rosenberg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our fruit test shows us everything we need to do a classification machine learning test. For each item with a *label* (apple, orange, lemon), we use a series of values to try to capture machine-understandable information about the item.  These values are a *feature representation* of the item in question.  The features themselves, as we can see above, can be numeric, true/false values, or a string in a set of predefined strings.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we had a new, unknown fruit?\n",
    "![fruit2](images/fruit2.png)\n",
    "Source: Andrew Rosenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our fruit test is an example of a *classification* task.  Classification allows you to predict a *categorical* value.  This is a type of *supervised* machine learning, meaning we know the labels ahead of time and can give them to the machine learning algorithm so that it can be trained to knows what the categories of our data are.  This way, when it comes time to give the previously algorithm previously unseen data, it knows which categories it's looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a thing about people who do work like this (Hannah?)\n",
    "- Viral Text (http://viraltexts.org/) - uses cluster analysis to find shared text propgated across networks\n",
    "- Hip Hop Vocabulary: https://pudding.cool/2017/02/vocabulary/ - inverse frequency project\n",
    "- Musical Genre Blurring in `Hamilton`: http://graphics.wsj.com/hamilton/ - genre classification\n",
    "- Mood Mining: http://mood-mining.herokuapp.com/ - mood classification/sentiment analysis\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get to coding!\n",
    "In this workshop we are going to *classify* two different sets of sentences from very different source material in the Brown corpus; one set of sentences from a corpus of news text, and the other set of sentences from a corpus of romance novel text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For a list of catorigies in the Brown corpus, use the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for cat in brown.categories():\n",
    "    print (cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the sentences from each corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_sent = brown.sents(categories=[\"news\"])\n",
    "romance_sent = brown.sents(categories=[\"romance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "### Practice 1:  Look at the first 5 sentences of each corpus\n",
    "- What do you notice about the format of the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "print (news_sent[:5])\n",
    "print ()\n",
    "print (romance_sent[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution": "shown",
    "solution2": "hidden",
    "solution_first": true
   },
   "outputs": [],
   "source": [
    "print (news_sent[:5])\n",
    "print ()\n",
    "print (romance_sent[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you notice about the format of the data above?\n",
    "Each sentence is already *tokenized* - split into a series of word and punctuation stringes, with whitespace removed. This saves us the time of having to do all of this work ourselves!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using data structures to manage data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start to organize our data, let's put these sentences into a pandas *DataFrame*, an object which has a format very similar to an Excel spreadsheet.  We will first make two spreadsheets (one for news, and one for romance), and then combine them into one.  We will also add the category each sentences came from, which will be our *labels* for each sentence and its associated feature representation (which we will build ourselves)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "ndf = pd.DataFrame({'sentence': news_sent,\n",
    "                    'label':'news'})\n",
    "rdf = pd.DataFrame({'sentence':romance_sent, \n",
    "                    'label':'romance'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining two spreadsheets into 1\n",
    "df = pd.concat([ndf, rdf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what this DataFrame looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So how many texts do we have of each type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we want to visualize that information?\n",
    "We first create a `figure` and `axes` on which to draw our charts using `plt.subplots()`. Each chart is one axes, and a figure can contain multiple charts. Our data is encapsulated in `df['label'].value_counts()`, which is itself a dataframe. We then tell the Pandas to visualize the dataframe as a bar chart using `.plot.bar(ax=ax, rot=0)`. The `ax` keyword tells Pandas which chart in the figure to plot, and the `rot` keyword controls the rotation of the x axis labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "_ = df['label'].value_counts().plot.bar(ax=ax, rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have slightly more news texts than romance texts, which we should keep in mind as we go ahead with classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What should we use as features for our data set?  What did we use as features for our fruit example before?\n",
    "![fruit3](images/fruit3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we are using sentences, how can we best repersent each sentence as a series of values?\n",
    "\n",
    "One idea is to use how many particular *parts of speech* the sentence contains.\n",
    "\n",
    "- Nouns: Most basically described as a person, place, or thing.  Counting nouns can help determine how many topics are being discussed in a sentence.\n",
    "- Adjectives: Descriptors of nouns (eg. \"yellow\", \"angry\", \"charming\").  Counting adjectives can help determine how often descriptive words are being added to nouns, which can demonstrate writing style.\n",
    "- Adverbs: Descriptors of verbs (eg. \"quickly\", \"hungrily\", \"annoyingly\").  Counting adverbs can help determine how often the manner of the verb is modified, which can also demonstrate writing style.\n",
    "\n",
    "#### Why might we want to use these parts of speech to distinguish between news sentences and romance sentences?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now compute all of the parts of speech on each sentence (row) in our dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "### Practice 2:  Get the part of speech tags for each sentence in the dataframe\n",
    "- Hint: \n",
    "~~~\n",
    "df['sentence']\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# compute parts of speech on each sentence (row)\n",
    "pos_all = pos_tag_sents(df['sentence'])\n",
    "print (pos_all[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's with those part of speech labels?  They aren't helpful at all!\n",
    "The Penn Tagset, which NLTK uses for it's part of speech tagger, is not particularly intuitive.  Fortunately, they provide code that allows you to check what different tags stand for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# troubleshooting: https://github.com/nltk/nltk/issues/919\n",
    "nltk.help.upenn_tagset(\"NN\")\n",
    "nltk.help.upenn_tagset(\"JJ\")\n",
    "nltk.help.upenn_tagset(\"RB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Troubleshooting**\n",
    "\n",
    "If you get an error that says something like:\n",
    "```\n",
    "Resource 'help/tagsets/PY3/upenn_tagset.pickle' not found.\n",
    "Please use the NLTK Downloader to obtain the resource: \n",
    ">>> nltk.download('tagsets')\n",
    "Searched in:\n",
    "- '/Users/user/nltk_data'\n",
    "- '/usr/share/nltk_data'\n",
    "- '/usr/local/share/nltk_data'\n",
    "- '/usr/lib/nltk_data'\n",
    "- '/usr/local/lib/nltk_data'\n",
    "- ''\n",
    "```\n",
    "You may need to manually unzip the `tagsets.zip` file found in `nltk_data/help/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a function that calculates our features for us \n",
    "#### (In this case, numbers of nouns, adjectives, and adverbs that appear in the sentence)\n",
    "\n",
    "Now we know the tags for the different parts of speech we want to count in each sentence.  Let's now write a function that will count the parts of speech to us, when given a part of speech tagged sentence (such as we have already in our DataFrame) and the part of speech we want to count (for example, \"NN\" to count the number of nouns in the sentence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countPOS(pos_tag_sent, POS):\n",
    "    pos_count = 0\n",
    "    all_pos_counts = []\n",
    "    for sentence in pos_tag_sent:\n",
    "        for word in sentence:\n",
    "            tag = word[1]\n",
    "            if tag [:2] == POS:  \n",
    "                pos_count = pos_count+1\n",
    "        all_pos_counts.append(pos_count)\n",
    "        pos_count = 0\n",
    "    return all_pos_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now call this function three different times, one for each of the parts of speech we are counting.  As we finish counting them, we put the results into the DataFrame, saving us the trouble of having to do so later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NN'] = countPOS(pos_all, 'NN')\n",
    "df['JJ'] = countPOS(pos_all, \"JJ\")\n",
    "df['RB'] = countPOS(pos_all, \"RB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So how many POS types do we have for each type of text?\n",
    "We can use the Pandas groupby function to aggregate our data based on unique values in any column of the data. Here we seperate our data into groups by the label type (news or romance) and then add together each texts count of nouns, adjectives, and adverbs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby('label').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize this data!\n",
    "What do you notice about the data?  Do you think our features will be good at predicting news and romance sentences?  Which features do you think will be the most useful?\n",
    "\n",
    "We can rotate the table using `.T` (transpose), which also changes the grouping of the data being plotted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(ncols=2,  figsize=(10,5))\n",
    "_ = df.groupby('label').sum().plot.bar(ax=ax1, rot=0)\n",
    "_ = df.groupby('label').sum().T.plot.bar(ax=ax2, color=['gray','hotpink'], rot=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "### Practice 3:  Save the dataframe to your computer as a csv file (comma separated value)\n",
    "\n",
    "- Hint:\n",
    "~~~\n",
    ".to_csv()\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"df_news_romance.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
