{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does unsupervised machine learning work?\n",
    "Unsupervised machine learning takes places in two steps - the *training* phase, and the *testing* phase. In the *training* phase, the algorithm develops a set of criteria for determining patterns in the data. One common unsupervised learning task is clustering, wherein the algorithm seeks to find groupings (labels) in a dataset. In the *training* phase, the clustering algorithm uses the training set to develop a set of criteria for deciding what class (label) an observation belongs to. \n",
    "\n",
    "In the testing phase, the criteria that tha algorithm developed in the training phase is applied to data that it had not seen during the training phase. This criteria is used to assign each observation in the *testing* data set to one of the classes discovered during the *training* phase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, lets consider the basket of fruit again:\n",
    "\n",
    "Object | Height | Width | Color  | Mass | Round ?\n",
    "-----  | -------| ------| -------| ---- | -------\n",
    "Apple  | 6cm    | 7cm   | Red    | 330g | TRUE   \n",
    "Orange | 6cm    | 7cm   | Orange | 330g | TRUE   \n",
    "Lemon  | 5cm    | 4cm   | Yellow | 150g | FALSE  \n",
    "\n",
    "In an unsupervised learning task, we don't know what fruit we have:\n",
    "\n",
    "Height | Width | Color  | Mass | Round ?\n",
    "-------| ------| -------| ---- | -------\n",
    " 6cm    | 7cm   | Red    | 330g | TRUE   \n",
    " 6cm    | 7cm   | Orange | 330g | TRUE   \n",
    " 5cm    | 4cm   | Yellow | 150g | FALSE  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we discover unknown patterns in the data?\n",
    "![algorithms_cheatsheet](images/algorithms_cheatsheet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is topic modeling using LDA?\n",
    "\n",
    "![diagram showing a collection of texts then an arrow going towards a black box named LDA. On the other side of the black box are two arrows. One is slightly tilted up and points toward three circles. Each circle is a topic and contains a sample of words in that topic. The other arrow is slightly titled down and points towards a document. In the document, words are annotated to indicate which topic they belong to (if any)](images/lda_diagram.png)\n",
    "\n",
    "In unsupervised learning tasks, the algorithm is not given any information about the type of observation they are seeing. As mentioned above, clustering tasks seek to find groupings in the dataset. One subset of these clustering tasks are topic extraction tasks, where the aim is to find common groupings of items across collections of items. One method of doing so is Latent Dirichlet allocation (LDA). In broad strokes, LDA extracts topics through the following method:<sup>1, 2</sup>\n",
    "\n",
    "1. Arbitrariy decide that there are 10 topics\n",
    "2. Select one document and randomly assign each word in the document to one of the 10 topics. \n",
    "3. Repeat 2 for all the other documents. This results in the same word being assigned to multiple topics.\n",
    "4. Compute\n",
    "    1. how many topics are in each document?\n",
    "    2. how many topic assignements are due to a given word?\n",
    "5. Take one word in one document and reassign it to a new topic and then repeat step 4.\n",
    "6. Repeat step 5 until the model stabilizes such that reassign topics does not change distributions. \n",
    "\n",
    "LDA yields the a set of words associated to each topic (4.2) and the mixture of topics associated to each document (4.1).\n",
    "    \n",
    "\n",
    "<sup>1</sup>[Introduction to Latent Dirichlet Allocation](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/) by Edward Chen \n",
    "\n",
    "<sup>2</sup>[The LDA Buffet is Now Open](http://www.matthewjockers.net/2011/09/29/the-lda-buffet-is-now-open-or-latent-dirichlet-allocation-for-english-majors/) by Matthew Jockers \n",
    "\n",
    "<sup>3</sup> Image is inspired by Christine Doig's PyTexas 2015 [\"Introduction to Topic Modeling\"](http://chdoig.github.io/pytexas2015-topic-modeling/#/) presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data in from a spreadsheet\n",
    "Lets take the data we just saved out and load it back into a dataframe so that we can do some analysis with it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "      <th>NN</th>\n",
       "      <th>JJ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>news</td>\n",
       "      <td>['The', 'Fulton', 'County', 'Grand', 'Jury', '...</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news</td>\n",
       "      <td>['The', 'jury', 'further', 'said', 'in', 'term...</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news</td>\n",
       "      <td>['The', 'September-October', 'term', 'jury', '...</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>news</td>\n",
       "      <td>['``', 'Only', 'a', 'relative', 'handful', 'of...</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>news</td>\n",
       "      <td>['The', 'jury', 'said', 'it', 'did', 'find', '...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                           sentence  NN  JJ\n",
       "0  news  ['The', 'Fulton', 'County', 'Grand', 'Jury', '...  11   2\n",
       "1  news  ['The', 'jury', 'further', 'said', 'in', 'term...  13   2\n",
       "2  news  ['The', 'September-October', 'term', 'jury', '...  16   2\n",
       "3  news  ['``', 'Only', 'a', 'relative', 'handful', 'of...   9   3\n",
       "4  news  ['The', 'jury', 'said', 'it', 'did', 'find', '...   5   3"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"df_news_romance.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for machine learning\n",
    "We're almost ready to do some machine learning!  First, we need to turn our sentences into the type of *feature vectors* LDA expects to work with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ['The', 'Fulton', 'County', 'Grand', 'Jury', '...\n",
       "1    ['The', 'jury', 'further', 'said', 'in', 'term...\n",
       "2    ['The', 'September-October', 'term', 'jury', '...\n",
       "3    ['``', 'Only', 'a', 'relative', 'handful', 'of...\n",
       "4    ['The', 'jury', 'said', 'it', 'did', 'find', '...\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentence'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words\n",
    "For LDA, we preprocess our data using sklearn's text feature extraction tools. In particular, we use the `CountVectorizer` which computes the frequency of each token in the document. We can omit outliers from our feature set using the `max_df` keyword to ignore words that occur more than some threshold and the `min_df` keyword to ignore words that occur below our set thresholds. The `CountVectorizer` can also be used to strip out stop words using the `stop_words` keyword argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tf_vectorizer = CountVectorizer(min_df =2, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(df['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CountVectorizer` processes the text such that `tf` is a sparse matrix containing the count of words in each document. One document in the Brown corpus is the following sentence: \n",
    ">Mrs. Robert O. Spurdle is chairman of the committee , which includes Mrs. James A. Moody , Mrs. Frank C. Wilkinson , Mrs. Ethel Coles , Mrs. Harold G. Lacy , Mrs. Albert W. Terry , Mrs. Henry M. Chance , 2d , Mrs. Robert O. Spurdle , Jr. , Mrs. Harcourt N. Trimble , Jr. , Mrs. John A. Moller , Mrs. Robert Zeising , Mrs. William G. Kilhour , Mrs. Hughes Cauffman , Mrs. John L. Baringer and Mrs. Clyde Newman .\n",
    "\n",
    "Via the `CountVectorizer` the stop words, punctuation, and very low frequency words have been removed. This yeilds the words and their counts listed below and visualized in the word cloud. \n",
    "\n",
    "```python\n",
    "{'2d': 1, 'albert': 1, 'chairman': 1, 'chance': 1, 'committee': 1,\n",
    " 'ethel': 1, 'frank': 1, 'harold': 1, 'henry': 1, 'hughes': 1, 'includes': 1, \n",
    " 'james': 1, 'john': 2, 'jr': 2, 'lacy': 1, 'moody': 1, 'mrs': 15, \n",
    " 'robert': 3, 'spurdle': 2, 'terry': 1, 'trimble': 1, 'william': 1}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Word cloud visualization, where the size of the word is relative to its frequency in a sentence, of \"Mrs. Robert O. Spurdle is chairman of the committee , which includes Mrs. James A. Moody , Mrs. Frank C. Wilkinson , Mrs. Ethel Coles , Mrs. Harold G. Lacy , Mrs. Albert W. Terry , Mrs. Henry M. Chance , 2d , Mrs. Robert O. Spurdle , Jr. , Mrs. Harcourt N. Trimble , Jr. , Mrs. John A. Moller , Mrs. Robert Zeising , Mrs. William G. Kilhour , Mrs. Hughes Cauffman , Mrs. John L. Baringer and Mrs. Clyde Newman .\"](images/countvect_wordcloud.png?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning data into train and test sets\n",
    "When you are partitioning your data into train and test sets, a good place to start is to use 75% of your data for training,and 25% of your data for testing.  We want as much training data as possible, while also having enough testing data to ensure that our trained classifier is generalizable across a number of examples.  This will also lead to more accurate evalutation of our trained classifier.\n",
    "\n",
    "Fortunately, sklearn has a function that will do exactly this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(fv, df['label'],\n",
    "                                                stratify=df['label'], \n",
    "                                                test_size=0.25,\n",
    "                                                random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We use the \"stratify\" argument because we have an uneven amount of training data; we have more news sentances than romance sentences.  By using stratify, we ensure that our classifier will take this data imbalence into account.\n",
    "\n",
    "\n",
    "- In this example, we are using a fixed random state, to ensure we will always get exactly the same value when we classify.  Adding this argument is unnecessary for most types of classification; we do it here to ensure our results do not vary slightly across runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the size of our train and test datasets using the `.shape` attribute of train and test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6790, 2) (6790,)\n",
      "(2264, 2) (2264,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do topic modeling with sklearn!\n",
    "One of the best things about sklearn is the simplicity of its syntax.\n",
    "\n",
    "To do machine learning with sklearn, follow these five steps (the function names remain the same, regardless of the algorithm you use!):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:  Import your desired algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create an instance of your machine learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LinearSVC(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3:  Fit your data to your classifier (train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=42, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, LinearSVC, which is a linear model for classification that separates classes using a line, a plane, or a hyperplane. The `classifier.fit` method searches for that line, plane, or hyperplane-which is also called the decision boundary. The dark gray line in the figure below is the decision boundary that the *LinearSVC* classifier found for this set of training data. All the data (dots) to the left of the gray line in the area with the orange background are classified as romance, while all the data to the right in the blue area are classified as news. The leftward skew of the classification space is due to the data being very dense and highly overlapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LinearSVC training data](images/training_boundary.png?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4:  Predict labels for unseen data (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Score!\n",
    "Evaluate the skill of the model by computing the \n",
    "* score: how many predicted labels are the same as the actual labels \n",
    "* confusion matrix: true positive, false positive, false negative, and true negative counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70759717314487636"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, our classifier can predict previously unseen news and data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[747, 409],\n",
       "       [253, 855]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|      |actual news | actual romance |\n",
    "|:--: | :--:| :--:|\n",
    "|predicted news | 747 | 409 |\n",
    "|predicted romance|253 | 855|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `LinearSVC` the `classifier.predict` decides which class a data point is in based on which side of the decision boundary, which is the gray line in the figure, the point falls on. Points in the orange area to the left of the gray line are classified as romance, while points in the blue area to the right of the gray line. Orange points in the blue area are romance texts that are misclassified as news texts, while blue points in the orange area are news texts that are misclassified as romance texts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LinearSVC Testing Data](images\\testing_boundary.png?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
