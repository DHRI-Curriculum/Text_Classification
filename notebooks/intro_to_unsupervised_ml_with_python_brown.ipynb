{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does unsupervised machine learning work?\n",
    "\n",
    "In supervised machine learning tasks, the data is assigned to some set of classes. For example, here we are given a dataset wherein each observation is a set of physical attributes of an object. In an supervised task, the object column acts as the labels. The algorithm then uses these existing separations in the data to develop criteria for classifying unknown observations in the data. \n",
    "\n",
    "label | Height | Width | Color  | Mass | Round ?\n",
    "-----  | -------| ------| -------| ---- | -------\n",
    "Apple  | 6cm    | 7cm   | Red    | 330g | TRUE   \n",
    "Orange | 6cm    | 7cm   | Orange | 330g | TRUE   \n",
    "Lemon  | 5cm    | 4cm   | Yellow | 150g | FALSE  \n",
    "\n",
    "In contrast, in an unsupervised machine learning task there either are no labels or that information is just treated as another attribute of the observation. In our fruit example, the object type is now just another characteristic of the observation, and often is altogether unknown:\n",
    "\n",
    "object | Height | Width | Color  | Mass | Round ?\n",
    "-----  | -------| ------| -------| ---- | -------\n",
    "Apple  | 6cm    | 7cm   | Red    | 330g | TRUE   \n",
    "Orange | 6cm    | 7cm   | Orange | 330g | TRUE   \n",
    "Lemon  | 5cm    | 4cm   | Yellow | 150g | FALSE  \n",
    " \n",
    "An unsupervised algorithm is not told how the data is structured or separated (barring parameter tuning); instead the algorithm goes looking for stucture and separation in the data. \n",
    "\n",
    "Clustering algorithms aim to group the observations in the data into categories (classes) based on some notion of how similar the observations are to each other. For example, given a basket of fruit, a clustering algorithm tries to group what it thinks are apples together into one class, and what it thinks are oranges into another. \n",
    "\n",
    "Dimension reduction techniques aim to decrease the number of rows and columns in a dataset based on some criteria such as which variables most separate the observations. For example, given the height, width, color, mass, and roundness of the fruit attributes, one dimension reduction algorithm will try to determine the minimum number of attributes needed to tell the fruit apart - can we tell it's an apple with just the mass and color? \n",
    "\n",
    "Generally speaking, in an unsupervised task there is no existing labeling to compare the results of the algorithm to; instead we often evaluate reliability through repeated experiments, computing the odds of our data being generated by our model, and visualizations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![algorithms_cheatsheet](../sections/images/algorithms_cheatsheet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data in from a spreadsheet\n",
    "Lets take the data we just saved out and load it back into a dataframe so that we can do some analysis with it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"df_news_romance.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for machine learning\n",
    "We're almost ready to do some machine learning!  First, we need to turn our sentences into the type of *feature vectors* LDA expects to work with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentence'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words\n",
    "For LDA, we preprocess our data using sklearn's text feature extraction tools. In particular, we use the `CountVectorizer` which computes the frequency of each token in the document. We can strip out stop words using the `stop_words` keyword argument. A keyword argument is an optional function parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tf_vectorizer = CountVectorizer(stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(df['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CountVectorizer` processes the text such that `tf` is a sparse matrix containing the count of words in each document. A matrix is a table of numbers, and a sparse matrix is a table where most of those numbers are 0. `tf` is mostly 0 because many words only appear in a handful of the many documents that make up our sample corpus. \n",
    "\n",
    ">Mrs. Robert O. Spurdle is chairman of the committee , which includes Mrs. James A. Moody , Mrs. Frank C. Wilkinson , Mrs. Ethel Coles , Mrs. Harold G. Lacy , Mrs. Albert W. Terry , Mrs. Henry M. Chance , 2d , Mrs. Robert O. Spurdle , Jr. , Mrs. Harcourt N. Trimble , Jr. , Mrs. John A. Moller , Mrs. Robert Zeising , Mrs. William G. Kilhour , Mrs. Hughes Cauffman , Mrs. John L. Baringer and Mrs. Clyde Newman .\n",
    "\n",
    "Via the `CountVectorizer` the stop words, punctuation, and very low frequency words have been removed. This yeilds the words and their counts listed below and visualized in the word cloud. \n",
    "\n",
    "```python\n",
    "{'2d': 1, 'albert': 1, 'baringer': 1, 'cauffman': 1, 'chairman': 1, 'chance': 1, 'clyde': 1, 'coles': 1, \n",
    " 'committee': 1, 'ethel': 1, 'frank': 1, 'harcourt': 1, 'harold': 1, 'henry': 1, 'hughes': 1, 'includes': 1, \n",
    " 'james': 1, 'john': 2, 'jr': 2, 'kilhour': 1, 'lacy': 1, 'moller': 1, 'moody': 1, 'mrs': 15, 'newman': 1, \n",
    " 'robert': 3, 'spurdle': 2, 'terry': 1, 'trimble': 1, 'wilkinson': 1, 'william': 1, 'zeising': 1}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Word cloud visualization, where the size of the word is relative to its frequency in a sentence, of \"Mrs. Robert O. Spurdle is chairman of the committee , which includes Mrs. James A. Moody , Mrs. Frank C. Wilkinson , Mrs. Ethel Coles , Mrs. Harold G. Lacy , Mrs. Albert W. Terry , Mrs. Henry M. Chance , 2d , Mrs. Robert O. Spurdle , Jr. , Mrs. Harcourt N. Trimble , Jr. , Mrs. John A. Moller , Mrs. Robert Zeising , Mrs. William G. Kilhour , Mrs. Hughes Cauffman , Mrs. John L. Baringer and Mrs. Clyde Newman .\"](../sections/images/countvect_wordcloud.png?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is topic modeling using LDA?\n",
    "\n",
    "![diagram showing a collection of texts then an arrow going towards a black box named LDA. On the other side of the black box are two arrows. One is slightly tilted up and points toward three circles. Each circle is a topic and contains a sample of words in that topic. The other arrow is slightly titled down and points towards a document. In the document, words are annotated to indicate which topic they belong to (if any)](../sections/images/lda_diagram.png)\n",
    "\n",
    "One subset of unsupervised learning tasks are topic extraction tasks, where the aim is to find common groupings of items across collections of items. One method of doing so is *Latent Dirichlet allocation (LDA)*. Latent Dirichlet Allocation is a way to model how topics are distributed over a corpus and words are distributed over a set of topics. \n",
    "\n",
    "In broad strokes, LDA extracts hidden (latent) topics via the following steps:<sup>1, 2</sup>\n",
    "\n",
    "1. Arbitrariy decide that there are 10 topics\n",
    "2. Select one document and randomly assign each word in the document to one of the 10 topics. \n",
    "3. Repeat 2 for all the other documents. This results in the same word being assigned to multiple topics.\n",
    "4. Compute\n",
    "    1. how many topics are in each document?\n",
    "    2. how many topic assignements are due to a given word?\n",
    "5. Take one word in one document and reassign it to a new topic and then repeat step 4.\n",
    "6. Repeat step 5 until the model stabilizes such that reassign topics does not change distributions. \n",
    "\n",
    "LDA yields the a set of words associated to each topic (4.2) and the mixture of topics associated to each document (4.1).\n",
    "    \n",
    "\n",
    "<sup>1</sup>[Introduction to Latent Dirichlet Allocation](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/) by Edward Chen \n",
    "\n",
    "<sup>2</sup>[The LDA Buffet is Now Open](http://www.matthewjockers.net/2011/09/29/the-lda-buffet-is-now-open-or-latent-dirichlet-allocation-for-english-majors/) by Matthew Jockers \n",
    "\n",
    "<sup>3</sup> Image is inspired by Christine Doig's PyTexas 2015 [\"Introduction to Topic Modeling\"](http://chdoig.github.io/pytexas2015-topic-modeling/#/) presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do topic modeling with sklearn!\n",
    "One of the best things about sklearn is the simplicity of its syntax.\n",
    "\n",
    "To do machine learning with sklearn, follow these five steps (the function names remain the same, regardless of the algorithm you use!):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:  Import your desired algorithm\n",
    "In this example, we will be using the [Latent Dirichlet Allocation](http://scikit-learn.org/stable/modules/decomposition.html#latentdirichletallocation) algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create an instance of your machine learning algorithm\n",
    " When creating an instance of sklearn's `LatentDirichletAllocation` algorithm to run on our data, we need to set paramters. `n_components` is the number of topics in the dataset and we set `random_state` to 42 so that this notebook is reproducible. Since the sentences happen to already have labels (either news or romance), lets see if LDA can also find those seperations by setting the number of topics to 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 2\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3:  Fit your data\n",
    "Using the `lda` object we set up above, we now apply (fit) the LDA algorithm to the bag of words we extracted from our sentences and had stored in the `tf` sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Transform your data\n",
    "We now want to model the documents in our corpus in terms of the topics discovered by the model. This is done using the `.transform` method of lda. This function yields the distribution of topics across the documents. The `document_topic` array contains the percentages of each topic found in each document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topic = lda.transform(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we visualize how much of each document is each topic - for example that document 1 is 10% topic A and 25% topic b. We choose an area chart because each band of the chart maps to a different category (in this case a unique topic). The width of each band in relation to the others illustrates how much of the document is thought to be about that topic relative to the others.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler\n",
    "import numpy as np\n",
    "\n",
    "colors = ['tab:green', 'tab:pink']\n",
    "topics = np.arange(10)\n",
    "num_docs = document_topic.shape[0]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "_ = ax.stackplot(range(num_docs), document_topic.T, labels=topics, colors=colors)\n",
    "_ = ax.set_xlim(0, num_docs)\n",
    "_ = ax.set_ylim(0,1)\n",
    "_ = ax.set_yticks([])\n",
    "_ = ax.set_xlabel(\"document\")\n",
    "_ = ax.legend(title=\"topic\", bbox_to_anchor=(1.06, 1), borderaxespad=0)\n",
    "fig.savefig(\"images/doc_topic.png\", bbox_inches = 'tight', pad_inches = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Print topics\n",
    "`lda.components_` is an array where each row is a topic, and each column roughly conatins the number of times that word was assigned to that topic, which is also the probability of that word being in that topic. To figure out which word is which column, we use the `get_feature_names())` function from `CountVectorizer`. The `argsort` function is used to return the indexes of the columns with the highest probabilities, which we then map into our collection of words. Here we print the top 5 words in each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 10\n",
    "topic_word  = lda.components_ \n",
    "words = np.array(tf_vectorizer.get_feature_names())\n",
    "for i, topic in enumerate(topic_word):\n",
    "    # sorting is in descending, so ::-1 reverses to ascending\n",
    "    sorted_idx = topic.argsort()[::-1]\n",
    "    print(i, words[sorted_idx][:num_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize these topics as lists sized by the frequency of the word and colored by the topic, as proposed by Allan Riddell in [Text Analysis with Topic Models for the Humanities and Social Sciences](https://de.dariah.eu/tatom/index.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# font size for word with largest share in corpus\n",
    "fontsize_base = 40/ np.max(topic_word)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 2), constrained_layout=True)\n",
    "\n",
    "for i, topic in enumerate(topic_word):\n",
    "    top_idx = topic.argsort()[::-1][:num_words]\n",
    "    top_words = words[top_idx]\n",
    "    top_share = topic[top_idx]\n",
    "    for j, (word, share) in enumerate(zip(top_words, top_share)):\n",
    "        ax.text(j, i/4,  word, fontsize=fontsize_base*share, color=colors[i])\n",
    "        \n",
    "#stretch the-axis to accommodate the words\n",
    "ax.set_xlim(0, num_words)\n",
    "ax.set_ylim(-.2, i/4+.2)\n",
    "ax.axis('off')\n",
    "#fig.subplots_adjust(hspace=-0)\n",
    "fig.savefig(\"../images/word_topic.png\", bbox_inches = 'tight', pad_inches = 0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Score!\n",
    "One method of evaluating a model is to compute the chance (probability) of the data we observed showing up in a dataset generated by the model. First we start with the modeled probability density function, which is the theoretical distribution of all topics in our model. We then use the *log likelihood* and the *perplexity* to evaluate the average odds of our observations occuring in the modeled distribution of words and topics. \n",
    "\n",
    "![the first is a plot of a normal (gaussian) curve, while the second is a plot of the log likelihood of the curve, and the third plot shows the perplexity of a distribution](../sections/images/xkcd_False.png?)\n",
    "\n",
    "Evaluate the success rate of the model by computing the \n",
    "* score: approximate log-liklihood - the higher the better\n",
    "* perplexity: exponent of the negative log likelihood - the lower the better\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Approximate Log Likelihood: {lda.score(tf)}')\n",
    "print(f'Perplexity: {lda.perplexity(tf)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Add supervision: compare topics to labels\n",
    "We can compare the results of our topic modeling to the labels we already have for the data. First we need to assign a label to each document based on which topic is most prevelant, which we can do using the `argmax` function since it returns the index (which maps directly to the topic) of the cell with the highest value. We then compare these topic based classes to the labes in our dataset. Given the sentences for each topic, we will make the assumption that topic 0 is news and topic 1 is romance. We can`argmax` returns the index of the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the location of the highest value in each column\n",
    "topic_class = document_topic.argmax(axis=1)\n",
    "topic_labels = np.empty(topic_class.shape, dtype=object)\n",
    "topic_labels[topic_class==0] = 'news'\n",
    "topic_labels[topic_class==1] = 'romance'\n",
    "topic_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use a confusion matrix to see if there is overlap between the topics and the labels. In a confusion matrix, the data is the counts of true positive, false positive, false negative, and true negative labeing. As a table, it is:\n",
    "\n",
    " |actual news | actual romance \n",
    ":--: | :--:| :--:\n",
    "predicted news || \n",
    "predicted romance| |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(df['label'], topic_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|actual news | actual romance \n",
    ":--: | :--:| :--:\n",
    "predicted news |2143|2480 \n",
    "predicted romance|1891 |2540"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "**Exercise 1**\n",
    "Unfortunately LDA doesn't seem to work all that well for this dataset. And nothing about the topics indicates a distinction between the romance and news texts...but we already saw that they didn't seem to be all that seperable. Can we get better results by expanding the corpus to include more texts of other types? Or by expanding each document so that it is longer than a sentence? \n",
    "\n",
    "**Exercise 2**\n",
    "Since topic modeling works better with longer texts, what topics do you get if you try to model:\n",
    "1. Moby Dick\n",
    "2. Pride and Prejudice\n",
    "3. Both together?\n",
    "4. A contemporary text like The Hunger Games"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
